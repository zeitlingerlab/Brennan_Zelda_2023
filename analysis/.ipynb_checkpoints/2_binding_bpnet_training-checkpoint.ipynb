{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: BPNet optimization, validation, and downstream processing \n",
    "\n",
    "The purpose of this notebook is to train a BPNet model we can use to interpret 6 TFs (Zelda, Dorsal, Twist, Caudal, Bicoid, and GAGA factor). To do this we will:\n",
    "\n",
    "1. Optimize the model based on input parameters/model architecture and select the best model based on correlations and auPRC.\n",
    "2. Once the best model architecture is selected, we will do a multi-fold training step to ensure that our training, validation, and test datasets are well established and stable. This will require running contribution and TF-MoDISco.\n",
    "3. Run TF-MoDISco on output `counts` contribution scores from (2).\n",
    "4. Compare the final model selected from (1) and sampled from (2) and compare it to obs/preds and replicate-replicate benchmarks to assess final model performances for each fold-change event.\n",
    "\n",
    "# Computational Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2022-08-18 14:08:13,263 [INFO] Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-08-18 14:08:13,268 [INFO] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.python.util import deprecation; deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "#Modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from glob import glob\n",
    "from itertools import product, compress\n",
    "from pybedtools import BedTool\n",
    "from keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from bpnet.utils import read_json, create_tf_session\n",
    "from bpnet.dataspecs import DataSpec\n",
    "from bpnet.datasets import StrandedProfile\n",
    "from bpnet.extractors import StrandedBigWigExtractor\n",
    "from bpnet.BPNet import BPNetSeqModel\n",
    "from bpnet.metrics import eval_profile\n",
    "\n",
    "#Setup\n",
    "os.chdir('/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/analysis/')\n",
    "# create_tf_session('0')\n",
    "%matplotlib inline\n",
    "\n",
    "#Variables\n",
    "prefix = 'ZDTBCG'\n",
    "figure_filepath = f'figures/2_binding_bpnet_training'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {figure_filepath}\n",
    "!mkdir -p bpnet/dataspec\n",
    "!mkdir -p bpnet/config\n",
    "!mkdir -p bpnet/pkl\n",
    "!mkdir -p tmp\n",
    "!mkdir -p tsv/optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the evaluation metrics computed in the training step of BPNet, we need to parse through a `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_eval_metrics(eval_path):\n",
    "    from bpnet.utils import read_json\n",
    "    eval_dict = read_json(eval_path)\n",
    "    df = pd.DataFrame([(k,k1,v1) for k,v in eval_dict.items() for k1,v1 in v.items()], columns = ['dataset','id','value'])\n",
    "    df = df.replace({'counts/':'counts//'}, regex=True) \n",
    "    df[['task','head','binsize','metric']] = df.id.str.split('/', expand=True) \n",
    "    df[(df['metric']=='auprc') | (df['metric']=='spearmanr')]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `dataspec.yaml` file based on the input bigwigs and peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_yml_dataspec(task_dict, fasta_file, output_yaml_path):\n",
    "    import yaml\n",
    "    yaml_dict = {\n",
    "        'fasta_file': f'{fasta_file}',\n",
    "        'task_specs': {}\n",
    "    }\n",
    "    \n",
    "    for task,bws in task_dict.items():\n",
    "        task_lower = task.lower()\n",
    "        task_dict = {\n",
    "                'tracks': list(bws.values()),\n",
    "                'peaks': f\"/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/narrowpeak/nexus_orer_mbt_{task_lower}_peaks.narrowpeak\"\n",
    "            }\n",
    "        yaml_dict['task_specs'][task] = task_dict\n",
    "    \n",
    "    with open(output_yaml_path, 'w') as yaml_file:\n",
    "        yaml.dump(yaml_dict, yaml_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a SGE-compatible training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_train_sge(output_sge_path, base_dir, training_bash_path):\n",
    "    \n",
    "    import sys    \n",
    "    sge_header = ['#$ -cwd', '#$ -S /bin/bash', '#$ -N optimize', '#$ -pe smp 80', \n",
    "                  '#$ -l h_vmem=2G', '#$ -l h_rt=24:00:00', '#$ -V', '#$ -p -5']\n",
    "    input_data = [f'cd {base_dir}']\n",
    "    dep_params = ['conda activate /home/mw2098/anaconda3/envs/bpnet-gpu']\n",
    "    cmd = [f'bash {training_bash_path}']\n",
    "    \n",
    "    cmds = sge_header + input_data + dep_params + cmd\n",
    "    with open(output_sge_path, mode='wt') as sge:\n",
    "        sge.write('\\n'.join(cmds))\n",
    "        sge.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a SGE-compatible training, contribution generation, and prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_train_contrib_predict_sge(output_sge_path, base_dir, output_model_dir, runid, input_gin, input_dataspec):\n",
    "    \n",
    "    import sys    \n",
    "    sge_header = ['#$ -cwd', '#$ -S /bin/bash', '#$ -N train_pred_contrib', '#$ -pe smp 80', \n",
    "                  '#$ -l h_vmem=2G', '#$ -l h_rt=24:00:00', '#$ -V', '#$ -p -20']\n",
    "    input_data = [f'cd {base_dir}']\n",
    "    dep_params = ['conda activate /home/mw2098/anaconda3/envs/bpnet-gpu']\n",
    "\n",
    "    #Train the model\n",
    "    model_train = [f'bpnet train --num-workers 16 --vmtouch --config {input_gin} --run-id {runid} --memfrac-gpu 1.0 {input_dataspec} {output_model_dir}']\n",
    "    \n",
    "    #Make predictions across the model\n",
    "    model_predict = [f'bpnet export-bw --contrib-method deeplift --flip-negative-strand --memfrac-gpu 1.0 {output_model_dir}/{runid} bpnet/preds/{runid}/bw/']\n",
    "\n",
    "\n",
    "    \n",
    "    #Collect SHAP scores across region\n",
    "    model_contrib = [f'bpnet contrib --method  deeplift --memfrac-gpu 1 {output_model_dir}/{runid} bpnet/preds/{runid}/contrib.h5',\n",
    "                     f'bpnet contrib --method  deeplift --memfrac-gpu 1 --shuffle-seq {output_model_dir}/{runid} bpnet/preds/{runid}/contrib_null.h5']\n",
    "    \n",
    "    #Write the script\n",
    "    cmds = sge_header + input_data + dep_params + model_train + model_predict + model_contrib    \n",
    "    \n",
    "    with open(output_sge_path, mode='wt') as sge:\n",
    "        sge.write('\\n'.join(cmds))\n",
    "        sge.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run BPNet TF-MoDISco across different tasks for counts contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_modisco_sh(output_sh_path, base_dir, tasks, runid):\n",
    "    \n",
    "    import sys    \n",
    "    sh_header = ['#!bin/bash']\n",
    "    input_data = [f'cd {base_dir}']\n",
    "    cmds = sh_header + input_data\n",
    "    \n",
    "    for t in tasks:\n",
    "        #Get Tf-MoDISco\n",
    "        modisco_run = [f'bpnet modisco-run --null-contrib-file bpnet/preds/{runid}/contrib_null.h5 \\\\',\n",
    "                       f'     --contrib-wildcard={t}/counts/pre-act --only-task-regions \\\\',\n",
    "                       f'     bpnet/preds/{runid}/contrib.h5 bpnet/modisco/{runid}/{t}_counts']\n",
    "\n",
    "        #Visualize logos and generate metadata\n",
    "        modisco_evaluate = [f'bpnet chip-nexus-analysis bpnet/modisco/{runid}/{t}_counts --footprint-width=800']\n",
    "\n",
    "    #     #Map motifs to peak-containing regions (i.e. each motifA will be mapped to TFA peaks, doesn't make sense to map to non-peak regions.)\n",
    "    #     cwm_map = [f'bpnet cwm-scan --contrib-file bpnet/preds/{runid}/contrib.h5 \\\\',\n",
    "    #                f'     bpnet/modisco/{runid}/{t}_counts bpnet/modisco/{runid}/{t}_counts/motif-instances-task-regions.tsv.gz']\n",
    "        \n",
    "        cmds = cmds + modisco_run\n",
    "        cmds = cmds + modisco_evaluate\n",
    "        \n",
    "    #Write the script\n",
    "    with open(output_sh_path, mode='wt') as sh:\n",
    "        sh.write('\\n'.join(cmds))\n",
    "        sh.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a custom set of regions, generate predictions across any regions for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bw_sge(output_sge_path, base_dir, model_dir, region_path, output_bw_path):\n",
    "    \n",
    "    import sys    \n",
    "    sge_header = ['#$ -cwd', '#$ -S /bin/bash', '#$ -N predict_custom', '#$ -pe smp 80', \n",
    "                  '#$ -l h_vmem=2G', '#$ -l h_rt=24:00:00', '#$ -V', '#$ -p -5']\n",
    "    input_data = [f'cd {base_dir}']\n",
    "    dep_params = ['conda activate /home/mw2098/anaconda3/envs/bpnet-gpu']\n",
    "    \n",
    "    cmd = [f'bpnet export-bw --memfrac-gpu 1 --contrib-method deeplift --regions {region_path} --flip-negative-strand {model_dir} {output_bw_path}']\n",
    "    cmds = sge_header + input_data + dep_params + cmd\n",
    "    with open(output_sge_path, mode='wt') as sge:\n",
    "        sge.write('\\n'.join(cmds))\n",
    "        sge.write('\\n')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters and input data\n",
    "\n",
    "Define combined filepaths and replicate-replicate filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, True], [True, True], [True, True], [True, True], [True, True], [True, True]]\n",
      "[[[True, True], [True, True]], [[True, True], [True, True]], [[True, True], [True, True]], [[True, True], [True, True]], [[True, True], [True, True]], [[True, True], [True, True]]]\n"
     ]
    }
   ],
   "source": [
    "combined_path_dict = {\n",
    "    'Zld': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_zld_mbt_nexus_combined_positive.bw',\n",
    "            'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_zld_mbt_nexus_combined_negative.bw'},\n",
    "    'Dl': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_dl_mbt_nexus_combined_positive.bw',\n",
    "           'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_dl_mbt_nexus_combined_negative.bw'},\n",
    "    'Twi': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_twi_mbt_nexus_combined_positive.bw',\n",
    "            'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_twi_mbt_nexus_combined_negative.bw'},\n",
    "    'Cad': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_cad_mbt_nexus_combined_positive.bw',\n",
    "            'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_cad_mbt_nexus_combined_negative.bw'},\n",
    "    'Bcd': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_bcd_mbt_nexus_combined_positive.bw',\n",
    "            'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_bcd_mbt_nexus_combined_negative.bw'},\n",
    "    'GAF': {'positive': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_gaf_mbt_nexus_combined_positive.bw',\n",
    "            'negative': f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/bw/dm6/nexus/combined/orer_gaf_mbt_nexus_combined_negative.bw'}\n",
    "}\n",
    "\n",
    "print([[os.path.exists(v2) for k2,v2 in v1.items()] for k1,v1 in combined_path_dict.items()])\n",
    "\n",
    "rep_path_dict = {\n",
    "    'Zld': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_zld_mbt_nexus_1_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_zld_mbt_nexus_1_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_zld_mbt_nexus_3_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_zld_mbt_nexus_3_negative.bw'\n",
    "        }\n",
    "    },\n",
    "    'Dl': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_dl_mbt_nexus_1_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_dl_mbt_nexus_1_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_dl_mbt_nexus_3_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_dl_mbt_nexus_3_negative.bw'\n",
    "        }\n",
    "    },\n",
    "    'Twi': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_twi_mbt_nexus_1_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_twi_mbt_nexus_1_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_twi_mbt_nexus_3_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_twi_mbt_nexus_3_negative.bw'\n",
    "        }\n",
    "    },\n",
    "    'Cad': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_cad_mbt_nexus_2_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_cad_mbt_nexus_2_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_cad_mbt_nexus_3_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_cad_mbt_nexus_3_negative.bw'\n",
    "        }\n",
    "    },\n",
    "    'Bcd': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_bcd_mbt_nexus_2_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_bcd_mbt_nexus_2_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_bcd_mbt_nexus_3_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_bcd_mbt_nexus_3_negative.bw'\n",
    "        }\n",
    "    },\n",
    "    'GAF': {\n",
    "        '1': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_gaf_mbt_nexus_1_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_gaf_mbt_nexus_1_negative.bw'\n",
    "        },\n",
    "        '2': {\n",
    "            'positive': f'../data/bw/dm6/nexus/individual/orer_gaf_mbt_nexus_4_positive.bw',\n",
    "            'negative': f'../data/bw/dm6/nexus/individual/orer_gaf_mbt_nexus_4_negative.bw'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "print([[[os.path.exists(v3) for k3,v3 in v2.items()] for k2,v2 in v1.items()] for k1,v1 in rep_path_dict.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataspec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_yml_dataspec(task_dict = combined_path_dict, \n",
    "                   fasta_file = f'/l/Zeitlinger/ZeitlingerLab/Manuscripts/Zelda_and_Nucleosomes/Analysis/data/indexes/bowtie2/dm6.fa',\n",
    "                   output_yaml_path = f'bpnet/dataspec/dataspec.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define config parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = list(combined_path_dict.keys())\n",
    "excl_chromosomes = ['chrY']\n",
    "valid_chromosomes = ['chr2L']\n",
    "test_chromosomes = ['chrX']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter assessment\n",
    "\n",
    "Here, we aim to look at several parameters:\n",
    "\n",
    "1. `filter length of nondilated convolutional layer`\n",
    "2. `filter length of deconvolutional layer`\n",
    "3. `number of filters`\n",
    "4. `learning rate`\n",
    "5. `lambda value weighting the count loss to the profile loss`\n",
    "6. `number of dilational layers`\n",
    "\n",
    "After determining optimal settings for these parameters, we will assess 1-2 in a \"dependent fashion\" e.g. how well each combination of these parameters influence one another.\n",
    "\n",
    "3-6 we will assess in an \"independent fashion\" e.g. how well each deviates from the default settings except for optional filter lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction of `conv` and `tconv` kernel size\n",
    "\n",
    "Because motifs are usually 7-8bp long, it makes sense to assess whether influencing the input filter length will be optimized as smaller. Additionally, because the fly genome is more condensed than the mouse genome (Avsec etal 2021), we might expect that the output convolutions would be more optimized when we focus the transpose convolutions on a more localized set of dilated layers at the end of the convolutions. Therefore, we will test smaller filter lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!python scripts/py/bpnet_train_as_grid_search.py --dataspec bpnet/dataspec/dataspec.yml \\\n",
    "--config bpnet/config/default.gin --output-directory bpnet/models/ -x 1 \\\n",
    "--filters '64' --conv-kernel-size '7,15,25' --tconv-kernel-size '7,15,25' --n-dil-layers '9' \\\n",
    "--loss-weight '10' --learning-rate '0.004' --seq-width '1000' --manually-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "current_date = datetime.now().strftime('%Y%m%d')\n",
    "for i in glob(f'tmp/{current_date}*_bpnet_grid_search_commands_tmp.sh'):\n",
    "    output_sge_path = i.replace('.sh', '.sge')\n",
    "    write_train_sge(output_sge_path = output_sge_path, base_dir = os.getcwd(), training_bash_path = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the resulting `.sge` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!qsub tmp/20220705155758_bpnet_grid_search_commands_tmp.sge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the `evaluation.valid.json` files from these steps and assess (1) Spearman counts correlation and (2) auPRC at 10bp as our respective counts and profile metrics to determine which is the best model. \n",
    "\n",
    "Because our focus is on Zelda as a pioneering factor, we want to prioritize Zelda as a task when selecting our model. Generally, task performance follow global patterns of model training, with marginal inter-task differences. But in order to make a systematic decision, we will fine tune our architecture to Zelda as this is our key TF of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>lr</th>\n",
       "      <th>lambda</th>\n",
       "      <th>n_dil_layers</th>\n",
       "      <th>conv_kernel_size</th>\n",
       "      <th>tconv_kernel_size</th>\n",
       "      <th>filters</th>\n",
       "      <th>auprc</th>\n",
       "      <th>spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>0.652736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.285018</td>\n",
       "      <td>0.647463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.265535</td>\n",
       "      <td>0.595999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.280683</td>\n",
       "      <td>0.599855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.273513</td>\n",
       "      <td>0.669322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.269454</td>\n",
       "      <td>0.652396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.241395</td>\n",
       "      <td>0.609339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.252636</td>\n",
       "      <td>0.614219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.255779</td>\n",
       "      <td>0.615999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric     lr  lambda  n_dil_layers  conv_kernel_size  tconv_kernel_size  \\\n",
       "0       0.004    10.0           9.0               7.0                7.0   \n",
       "1       0.004    10.0           9.0               7.0               15.0   \n",
       "2       0.004    10.0           9.0               7.0               25.0   \n",
       "3       0.004    10.0           9.0              15.0                7.0   \n",
       "4       0.004    10.0           9.0              15.0               15.0   \n",
       "5       0.004    10.0           9.0              15.0               25.0   \n",
       "6       0.004    10.0           9.0              25.0                7.0   \n",
       "7       0.004    10.0           9.0              25.0               15.0   \n",
       "8       0.004    10.0           9.0              25.0               25.0   \n",
       "\n",
       "metric  filters     auprc  spearmanr  \n",
       "0          64.0  0.286703   0.652736  \n",
       "1          64.0  0.285018   0.647463  \n",
       "2          64.0  0.265535   0.595999  \n",
       "3          64.0  0.280683   0.599855  \n",
       "4          64.0  0.273513   0.669322  \n",
       "5          64.0  0.269454   0.652396  \n",
       "6          64.0  0.241395   0.609339  \n",
       "7          64.0  0.252636   0.614219  \n",
       "8          64.0  0.255779   0.615999  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_path = glob(f'bpnet/models/seq_width1000-lr0.004-lambda10-n_dil_layers9-conv_kernel_size*-tconv_kernel_size*-filters64/evaluation.valid.json')\n",
    "evals_dict = {i.split('/')[2]: collect_eval_metrics(i) for i in evals_path}\n",
    "\n",
    "#Convert to pd.df\n",
    "evals_df = pd.DataFrame()\n",
    "for k,v in evals_dict.items():\n",
    "    v['model'] = k\n",
    "    evals_df = evals_df.append(v)\n",
    "\n",
    "#Integrate model information\n",
    "model_params = ['seq_width','lr','lambda','n_dil_layers','conv_kernel_size','tconv_kernel_size','filters']\n",
    "evals_df[model_params] = evals_df.model.str.split('-', expand=True)\n",
    "model_params = model_params + ['binsize']\n",
    "evals_df[model_params] = evals_df[model_params].replace('[-+A-Za-z]','', regex = True).replace('_','', regex = True).replace('=','', regex = True)\n",
    "evals_df[model_params] = evals_df[model_params].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "#Filter evaluation features to our features of interest\n",
    "binsize = 10\n",
    "evals_df = evals_df[evals_df['dataset']=='valid-peaks']\n",
    "evals_df = evals_df[evals_df['task']=='Zld']\n",
    "evals_df['binsize'] = evals_df['binsize'].fillna(binsize)\n",
    "evals_df = evals_df[evals_df['binsize']==binsize]\n",
    "evals_df = evals_df[evals_df['metric'].str.contains('^auprc$|spearmanr', regex = True)]\n",
    "evals_df = evals_df.drop(['id', 'dataset', 'model', 'task', 'head', 'binsize', 'seq_width'], axis=1)\n",
    "evals_df = evals_df.reset_index(drop = True)\n",
    "filter_length_evals_df = evals_df = pd.pivot_table(evals_df,\n",
    "                                                   index=['lr', 'lambda', 'n_dil_layers', 'conv_kernel_size', 'tconv_kernel_size', 'filters'],\n",
    "                                                   columns='metric', values='value').reset_index()\n",
    "\n",
    "filter_length_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length_evals_df.to_csv('tsv/optimization/bpnet_optimization_of_filter_lengths.tsv.gz', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the values are quite similar, there is not a large margin here. So we will work with convolutional/transpose convolutional filter lengths of 7bp when assessing further parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing parameters \"independently\"\n",
    "\n",
    "Number of filters, learning rate, and lambda can be assessed in an independent fashion relative to the recommended defaults. Because BPNet was shown to be stable in (Avsec et al 2021), we will not do a full grid search. However, optimization as a response of changing the filter lengths is ideal, so parameters were selected above to test from existing defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "#Modify learning rate\n",
    "!python scripts/py/bpnet_train_as_grid_search.py --dataspec bpnet/dataspec/dataspec.yml \\\n",
    "--config bpnet/config/default.gin --output-directory bpnet/models/ -x 1 --manually-run --num-workers 16 \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '9' --loss-weight '10' --learning-rate '0.01,0.004,0.001,0.0004' --seq-width '1000'\n",
    "\n",
    "#Modify counts:profile loss ratio\n",
    "!python scripts/py/bpnet_train_as_grid_search.py --dataspec bpnet/dataspec/dataspec.yml \\\n",
    "--config bpnet/config/default.gin --output-directory bpnet/models/ -x 1 --manually-run --num-workers 16 \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '9' --loss-weight '1,10,100,1000' --learning-rate '0.004' --seq-width '1000'\n",
    "\n",
    "#Modify number of filters in convolutional layers\n",
    "!python scripts/py/bpnet_train_as_grid_search.py --dataspec bpnet/dataspec/dataspec.yml \\\n",
    "--config bpnet/config/default.gin --output-directory bpnet/models/ -x 1 --manually-run --num-workers 16 \\\n",
    "--filters '64,128,256' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '9' --loss-weight '10' --learning-rate '0.004' --seq-width '1000'\n",
    "\n",
    "#Modify number of convolutional layers.\n",
    "!python scripts/py/bpnet_train_as_grid_search.py --dataspec bpnet/dataspec/dataspec.yml \\\n",
    "--config bpnet/config/default.gin --output-directory bpnet/models/ -x 1 --manually-run --num-workers 16 \\\n",
    "--filters '64' --conv-kernel-size '7' --tconv-kernel-size '7' --n-dil-layers '7,9,11' --loss-weight '10' --learning-rate '0.004' --seq-width '1000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a loop that will automatically generate an SGE file from these scripts that are saved under `tmp/{date}*bpnet_grid_search_commands_tmp.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "current_date = datetime.now().strftime('%Y%m%d')\n",
    "for i in glob(f'tmp/{current_date}*_bpnet_grid_search_commands_tmp.sh'):\n",
    "    output_sge_path = i.replace('.sh', '.sge')\n",
    "    write_train_sge(output_sge_path = output_sge_path, base_dir = os.getcwd(), training_bash_path = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the resulting `.sge` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!qsub tmp/20220705160713_bpnet_grid_search_commands_tmp.sge #learning rate\n",
    "!qsub tmp/20220705160720_bpnet_grid_search_commands_tmp.sge #lambda\n",
    "!qsub tmp/20220705160724_bpnet_grid_search_commands_tmp.sge #filters\n",
    "!qsub tmp/20220705160730_bpnet_grid_search_commands_tmp.sge #dilational layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the `evaluation.valid.json` files from these steps and assess (1) Spearman counts correlation and (2) auPRC at 10bp as our respective counts and profile metrics to determine which is the best model. \n",
    "\n",
    "Because our focus is on Zelda as a pioneering factor, we want to prioritize Zelda as a task when selecting our model. Generally, task performance follow global patterns of model training, with marginal inter-task differences. But in order to make a systematic decision, we will fine tune our architecture to Zelda as this is our key TF of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>lr</th>\n",
       "      <th>lambda</th>\n",
       "      <th>n_dil_layers</th>\n",
       "      <th>conv_kernel_size</th>\n",
       "      <th>tconv_kernel_size</th>\n",
       "      <th>filters</th>\n",
       "      <th>auprc</th>\n",
       "      <th>spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0004</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.256612</td>\n",
       "      <td>0.576783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.274359</td>\n",
       "      <td>0.632176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.261431</td>\n",
       "      <td>0.633172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.631287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.286703</td>\n",
       "      <td>0.652736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.295199</td>\n",
       "      <td>0.689881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.301094</td>\n",
       "      <td>0.713390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.289098</td>\n",
       "      <td>0.669558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.259024</td>\n",
       "      <td>0.676986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0040</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.202913</td>\n",
       "      <td>0.707356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.204403</td>\n",
       "      <td>0.586071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric      lr  lambda  n_dil_layers  conv_kernel_size  tconv_kernel_size  \\\n",
       "0       0.0004    10.0           9.0               7.0                7.0   \n",
       "1       0.0010    10.0           9.0               7.0                7.0   \n",
       "2       0.0040     1.0           9.0               7.0                7.0   \n",
       "3       0.0040    10.0           7.0               7.0                7.0   \n",
       "4       0.0040    10.0           9.0               7.0                7.0   \n",
       "5       0.0040    10.0           9.0               7.0                7.0   \n",
       "6       0.0040    10.0           9.0               7.0                7.0   \n",
       "7       0.0040    10.0          11.0               7.0                7.0   \n",
       "8       0.0040   100.0           9.0               7.0                7.0   \n",
       "9       0.0040  1000.0           9.0               7.0                7.0   \n",
       "10      0.0100    10.0           9.0               7.0                7.0   \n",
       "\n",
       "metric  filters     auprc  spearmanr  \n",
       "0          64.0  0.256612   0.576783  \n",
       "1          64.0  0.274359   0.632176  \n",
       "2          64.0  0.261431   0.633172  \n",
       "3          64.0  0.267574   0.631287  \n",
       "4          64.0  0.286703   0.652736  \n",
       "5         128.0  0.295199   0.689881  \n",
       "6         256.0  0.301094   0.713390  \n",
       "7          64.0  0.289098   0.669558  \n",
       "8          64.0  0.259024   0.676986  \n",
       "9          64.0  0.202913   0.707356  \n",
       "10         64.0  0.204403   0.586071  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_path = glob(f'bpnet/models/seq_width1000-lr*-lambda*-n_dil_layers*-conv_kernel_size7-tconv_kernel_size7-filters*/evaluation.valid.json')\n",
    "evals_dict = {i.split('/')[2]: collect_eval_metrics(i) for i in evals_path}\n",
    "\n",
    "#Convert to pd.df\n",
    "evals_df = pd.DataFrame()\n",
    "for k,v in evals_dict.items():\n",
    "    v['model'] = k\n",
    "    evals_df = evals_df.append(v)\n",
    "\n",
    "#Integrate model information\n",
    "model_params = ['seq_width','lr','lambda','n_dil_layers','conv_kernel_size','tconv_kernel_size','filters']\n",
    "evals_df[model_params] = evals_df.model.str.split('-', expand=True)\n",
    "model_params = model_params + ['binsize']\n",
    "evals_df[model_params] = evals_df[model_params].replace('[-+A-Za-z]','', regex = True).replace('_','', regex = True).replace('=','', regex = True)\n",
    "evals_df[model_params] = evals_df[model_params].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "#Filter evaluation features to our features of interest\n",
    "binsize = 10\n",
    "evals_df = evals_df[evals_df['dataset']=='valid-peaks']\n",
    "evals_df = evals_df[evals_df['task']=='Zld']\n",
    "evals_df['binsize'] = evals_df['binsize'].fillna(binsize)\n",
    "evals_df = evals_df[evals_df['binsize']==binsize]\n",
    "evals_df = evals_df[evals_df['metric'].str.contains('^auprc$|spearmanr', regex = True)]\n",
    "evals_df = evals_df.drop(['id', 'dataset', 'model', 'task', 'head', 'binsize', 'seq_width'], axis=1)\n",
    "evals_df = evals_df.reset_index(drop = True)\n",
    "other_evals_df = evals_df = pd.pivot_table(evals_df,\n",
    "                                                   index=['lr', 'lambda', 'n_dil_layers', 'conv_kernel_size', 'tconv_kernel_size', 'filters'],\n",
    "                                                   columns='metric', values='value').reset_index()\n",
    "\n",
    "other_evals_df.sort_values(['lr', 'lambda', 'n_dil_layers', 'filters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_evals_df.to_csv('tsv/optimization/bpnet_optimization_of_parameters.tsv.gz', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, for each comparison, we will select the best of each feature to compile a final model architecture. \n",
    "\n",
    "## Summary of parameter results\n",
    "\n",
    "Here, we can see the results of the following parameters:\n",
    "+ `filters`: 256 is the optimal filter set, though 128 filters performs similarly.\n",
    "+ `n_dil_layers`: all layers perform similarly, with minor improvements at 9. \n",
    "+ `lambda`: a lambda 100 provides a better auPRC and counts correlation.\n",
    "+ `learning_rate`: a learning rate of around 0.004 performed the best for profile and auprc metrics.\n",
    "+ `conv` and `tconv`: as we saw above smaller `conv` values result in minor improvements in auPRC.\n",
    "\n",
    "Given these findings, the model that will perform the best for Zelda is: `seq_width1000-lr0.004-lambda100-n_dil_layers9-conv_kernel_size7-tconv_kernel_size7-filters256`, which needs to be trained. We will call it `optimized_model` for a prefix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-fold validation\n",
    "\n",
    "We will run two other models with the same optimized architectures with the training/validation/test chromosomes shuffled around. The purpose of this analysis is to establish that the regions selected for training in our above model are representative of the entire dataset. After training, in subsequent documents we will record running TF-MoDISco to show the motifs learned are similar. \n",
    "\n",
    "First, we will train the models, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(1,4):\n",
    "    write_train_contrib_predict_sge(f'tmp/train_contrib_pred_fold{fold}.sge', \n",
    "                                    base_dir = os.getcwd(), \n",
    "                                    output_model_dir = 'bpnet/models/optimized_model', \n",
    "                                    runid = f'fold{fold}', \n",
    "                                    input_gin = f'bpnet/config/optimized_chrom_fold{fold}.gin', \n",
    "                                    input_dataspec = 'bpnet/dataspec/dataspec.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!qsub tmp/train_contrib_pred_fold1.sge \n",
    "!qsub tmp/train_contrib_pred_fold2.sge\n",
    "!qsub tmp/train_contrib_pred_fold3.sge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare model metrics for the task Zelda to see if they are significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>model</th>\n",
       "      <th>auprc</th>\n",
       "      <th>spearmanr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold1</td>\n",
       "      <td>0.284964</td>\n",
       "      <td>0.753756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold2</td>\n",
       "      <td>0.317472</td>\n",
       "      <td>0.731291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold3</td>\n",
       "      <td>0.302352</td>\n",
       "      <td>0.685476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric  model     auprc  spearmanr\n",
       "0       fold1  0.284964   0.753756\n",
       "1       fold2  0.317472   0.731291\n",
       "2       fold3  0.302352   0.685476"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_path = glob(f'bpnet/models/optimized_model/*/evaluation.valid.json')\n",
    "evals_dict = {i.split('/')[3]: collect_eval_metrics(i) for i in evals_path}\n",
    "\n",
    "#Convert to pd.df\n",
    "evals_df = pd.DataFrame()\n",
    "for k,v in evals_dict.items():\n",
    "    v['model'] = k\n",
    "    evals_df = evals_df.append(v)\n",
    "\n",
    "#Integrate model information\n",
    "\n",
    "#Filter evaluation features to our features of interest\n",
    "binsize = 10\n",
    "evals_df = evals_df[evals_df['dataset']=='valid-peaks']\n",
    "evals_df = evals_df[evals_df['task']=='Zld']\n",
    "evals_df['binsize'] = evals_df['binsize'].str.replace('binsize=', '').replace('', np.NaN).fillna(binsize).astype(int)\n",
    "evals_df = evals_df[evals_df['binsize']==binsize]\n",
    "evals_df[['task', 'head', 'filler', 'metric']] = evals_df.id.str.split('/', expand=True)\n",
    "evals_df = evals_df[evals_df['metric'].str.contains('^auprc$|spearmanr', regex = True)]\n",
    "evals_df = evals_df.drop(['id', 'dataset', 'task', 'head', 'binsize', 'filler'], axis=1)\n",
    "evals_df = evals_df.reset_index(drop = True)\n",
    "\n",
    "fold_evals_df = evals_df = pd.pivot_table(evals_df,index=['model'],columns='metric', values='value').reset_index()\n",
    "fold_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_evals_df.to_csv('tsv/optimization/bpnet_evaluation_of_foldevals.tsv.gz', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance metrics do not look largely different. Best way of determining whether the model systematically learned the correct features now is to run TF-MoDISco to ensure that all the motif grammar that we expect from TF binding behavior is returned as well.\n",
    "\n",
    "Now that these models have been trained, we will run TF-MoDISco in later documents and also generate data to compare to these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run TF-MoDISco \n",
    "\n",
    "Now that the different models have been optimized, trained in different folds, and the contributions have been generated, we can run TF-MoDISco to group the important sequence features into logos, then map the motifs back to the genome.\n",
    "\n",
    "For this run of TF-MoDISco, we have to pick between `counts` and `profile` contribution output heads. Because we are mostly interested in the magnitude of binding as it relates to downstream accessibility analysis, and because the fly ChIP-nexus data is always less complex (due to a more condensed genome), we will be using the `counts` contribution to cluster and map our motifs. The `counts` and `profile` contribution scores are very similar to one another, but we will select the one that matches with our intended scientific goal more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(1,4):\n",
    "    write_modisco_sh(f'tmp/modisco_fold{fold}.sh', \n",
    "                     base_dir = os.getcwd(),\n",
    "                     tasks = tasks,\n",
    "                     runid = f'fold{fold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For overviews of each TF-MoDISco run, go to `analysis/bpnet/modisco/*/*/modisco-chip.html`. This will show all the logos returned, their numbers, and relevant metaplots. \n",
    "\n",
    "In `3_*.Rmd` we will map the pertinent motifs back to the genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract TF-MoDISco logos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modisco_cmds = ['#!bin/bash']\n",
    "for fold in range(1,4):\n",
    "    for t in tasks:\n",
    "        modisco_cmds.append(f'python scripts/py/bpnet_extract_modisco_logos.py --ic_threshold 0.08 --modisco_model_file bpnet/modisco/fold{fold}/{t}_counts/modisco.h5 --output_h5 bpnet/modisco/fold{fold}/{t}_counts/modisco_logos.h5')\n",
    "with open(f'tmp/ZDTBCG_modisco_logos.sh', \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(modisco_cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!bash tmp/ZDTBCG_modisco_logos.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output enhancer predictions\n",
    "\n",
    "Because stitching between regions can be somewhat cumbersome with multiple regions that are overlapping in the middle of an enhancer, we want to obtain the highest quality predictions across our key regions of interest. For this, we will generate a set of predictions in the form of .bw files that will show us enhancers centered on the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_bw_sge(output_sge_path = 'tmp/ZDTBCG_export_contrib_bw_enhancers.sge', \n",
    "             base_dir = os.getcwd(), \n",
    "             model_dir = 'bpnet/models/optimized_model/fold1', \n",
    "             region_path = 'bed/enhancers/enhancers_for_models.bed',\n",
    "             output_bw_path = 'bpnet/preds/fold1/bw/enhancers_only/enhancer_preds_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "qsub tmp/ZDTBCG_export_contrib_bw_enhancers.sge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of best model with observed/replicate quality metrics \n",
    "\n",
    "Here, we will generate plots for each of the optimized model folds to ensure that they are performing adequately as compared to replicate-replicate comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect coverage from predictions, observed, and replicate values\n",
    "\n",
    "Here, we want to compare replicate and metapeak quality metrics with the models trained below. Keep in mind that we will remove regions not associated with each task in order to ensure that we aren't predicting regions that are not of interest for particular tasks.\n",
    "\n",
    "First, collect data and regions for coverage extraction. We created this as a python script so we could run it in the background, as collecting this data takes time. It will be saved as an intermediate .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!python scripts/collect_obs_vs_pred_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bpnet/pkl/obs_vs_pred_metrics.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with this data collected, generate correlation and auPRC plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate counts correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the observed counts already have the `log` applied to them based off of the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = list(combined_path_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpnet.metrics import pearsonr,spearmanr\n",
    "\n",
    "corr_df = pd.DataFrame()\n",
    "counts_df = pd.DataFrame()\n",
    "for fold in range(1,4):\n",
    "    for task in tqdm(tasks):\n",
    "        #Filtering regions such that they belong to the correct task at hand\n",
    "        ytrue_logcounts_filtered = np.mean(np.log(np.exp(data_dict[f'fold{fold}']['y_obs'][f'{task}/counts'])+ 1), axis = 1)\n",
    "        ypred_logcounts_filtered = np.mean(np.log(np.sum(data_dict[f'fold{fold}']['y_pred'][task], axis = 1) + 1), axis = 1)\n",
    "        c_df = pd.DataFrame([ytrue_logcounts_filtered, ypred_logcounts_filtered])\n",
    "        c_df = c_df.transpose()\n",
    "        c_df.columns = ['obs','pred']\n",
    "        c_df['task'] = task\n",
    "        c_df['fold'] = f'fold{fold}'\n",
    "        counts_df = counts_df.append(c_df)\n",
    "        \n",
    "        pearson = pearsonr(ytrue_logcounts_filtered, ypred_logcounts_filtered)\n",
    "        spearman = spearmanr(ytrue_logcounts_filtered, ypred_logcounts_filtered)\n",
    "        \n",
    "        df = pd.DataFrame([pearson, spearman, task]).transpose()\n",
    "        df.columns = ['pearson_corr','spearman_corr','task']\n",
    "        df['fold'] = f'fold{fold}'\n",
    "        corr_df = corr_df.append(df)\n",
    "corr_df['txt'] = ['corr_s: ' + str(np.round(row.spearman_corr, 2)) + '\\ncorr_p: ' + str(np.round(row.pearson_corr, 2)) for i,row in corr_df.iterrows()]\n",
    "counts_df.to_csv('tsv/optimization/bpnet_counts_across_model_folds.tsv.gz', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the counts correlations with their correlation scores reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plotnine.options.figure_size = (16,9)\n",
    "\n",
    "corr_plot = (ggplot(data = counts_df, mapping = aes('obs','pred'))+\n",
    "    geom_point(aes(color = 'task'), size = .2)+\n",
    "    geom_text(data = corr_df, mapping = aes(x = 1.5, y = 10, label = 'txt'))+\n",
    "    scale_x_continuous(name = 'log(obs_counts + 1)')+\n",
    "    scale_y_continuous(name = 'log(pred_counts + 1)')+\n",
    "    scale_color_manual(values = sns.color_palette('viridis', n_colors = len(tasks)).as_hex())+\n",
    "    facet_grid('fold ~ task')+\n",
    "    ggtitle(f'Correlations across test set')+\n",
    "    theme_classic())\n",
    "\n",
    "corr_plot.save(f'{figure_filepath}/corr_test_regions.png', height = 9, width = 16)\n",
    "corr_plot.save(f'{figure_filepath}/corr_test_regions.pdf', height = 9, width = 16)\n",
    "print(corr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate replicate and metapeak auPRC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calulcate auPRC values for this model\n",
    "pos_min_thresh = 0.015\n",
    "neg_max_thresh = 0.005\n",
    "\n",
    "auprc_df = pd.DataFrame()\n",
    "for fold in range(1,4):\n",
    "    for task in tqdm(tasks):\n",
    "\n",
    "        #Filtering regions if we decide to do this method.\n",
    "        # task_idx = np.where(data['metadata']['interval_from_task'] == f'{task}')[0] #only get task-specific regions from tasks\n",
    "        task_idx = range(data_dict[f'fold{fold}']['y_obs'][f'{task}/profile'].shape[0]) #get all regions from tasks\n",
    "\n",
    "        #Observed versus predicted signal\n",
    "        o_vs_p_df = eval_profile(yt = data_dict[f'fold{fold}']['y_obs'][f'{task}/profile'][task_idx], \n",
    "                                 yp = data_dict[f'fold{fold}']['y_pred'][task][task_idx],\n",
    "                                 pos_min_threshold=pos_min_thresh, neg_max_threshold=neg_max_thresh,\n",
    "                                 required_min_pos_counts=2.5, binsizes=[1, 2, 4, 10, 20, 50, 100])\n",
    "        o_vs_p_df['type']='obs_vs_pred'\n",
    "\n",
    "        #Replicate 1 vs replicate 2\n",
    "        r1_vs_r2_df = eval_profile(yt = data_dict[f'fold{fold}']['rep1_obs'][f'{task}'][task_idx], \n",
    "                                   yp = data_dict[f'fold{fold}']['rep2_obs'][f'{task}'][task_idx],\n",
    "                                   pos_min_threshold=pos_min_thresh, neg_max_threshold=neg_max_thresh,\n",
    "                                   required_min_pos_counts=2.5, binsizes=[1, 2, 4, 10, 20, 50, 100])\n",
    "        r1_vs_r2_df['type']='rep1_vs_rep2'\n",
    "\n",
    "        #Observed versus metpeak signal\n",
    "        o_vs_mp_df = eval_profile(yt = data_dict[f'fold{fold}']['y_obs'][f'{task}/profile'][task_idx], \n",
    "                                  yp = data_dict[f'fold{fold}']['y_mp'][f'{task}'][task_idx],\n",
    "                                  pos_min_threshold=pos_min_thresh, neg_max_threshold=neg_max_thresh,\n",
    "                                  required_min_pos_counts=2.5, binsizes=[1, 2, 4, 10, 20, 50, 100])\n",
    "        o_vs_mp_df['type']='obs_vs_metapeak'\n",
    "\n",
    "        #Observed versus randomly shuffled signal\n",
    "        o_vs_r_df = o_vs_p_df.copy()\n",
    "        o_vs_r_df['auprc'] = o_vs_r_df['random_auprc']\n",
    "        o_vs_r_df['type']='obs_vs_random'\n",
    "\n",
    "        df = pd.concat([o_vs_p_df, r1_vs_r2_df, o_vs_mp_df, o_vs_r_df])\n",
    "        df['task'] = task\n",
    "        df['fold'] = f'fold{fold}'\n",
    "        auprc_df = auprc_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auprc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the auPRC values of actual data for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plotnine.options.figure_size = (16,9)\n",
    "\n",
    "auprc_df['type'] = pd.Categorical(auprc_df['type'], \n",
    "                                  categories=['obs_vs_random', 'obs_vs_pred', 'rep1_vs_rep2', 'obs_vs_metapeak'], \n",
    "                                  ordered=False)\n",
    "\n",
    "auprc_plot = (ggplot(data = auprc_df, mapping = aes(x = 'binsize',y='auprc'))+\n",
    "    geom_point(mapping = aes(color = 'type'))+\n",
    "    geom_line(mapping = aes(color = 'type'))+\n",
    "    scale_color_manual(values = ['#000000', '#b2182b', '#2166ac', '#fc8b01'])+\n",
    "    facet_grid('fold ~ task')+\n",
    "    ggtitle('ZDTBCG auPRC across task-specific regions')+\n",
    "    theme_classic())\n",
    "auprc_plot.save(f'{figure_filepath}/auprc_test_regions.png', height = 9, width = 16)\n",
    "auprc_plot.save(f'{figure_filepath}/auprc_test_regions.pdf', height = 9, width = 16)\n",
    "print(auprc_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpnet",
   "language": "python",
   "name": "bpnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
